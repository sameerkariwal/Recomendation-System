{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a model evaluation code for evaluating and comparing all models performances\n",
    "#### major metrics involves:  hit rate and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from data_processing.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SumitJain\\Documents\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of users: 1895\n",
      "no. of users with atleast 5 consumptions: 1140\n",
      "no. of interactions: 72312\n",
      "no. of interactions from users with at least 5 engagement: 69868\n",
      "no. of unique user/item interactions: 39106\n",
      "engagement in train set: 33240\n",
      "engagement in on test set: 5866\n",
      "               consumer_id              item_id  type_weightage\n",
      "2103  -8550167523008133722  -454649054276160610        1.000000\n",
      "31618  4670267857749552625  6587635730509289343        1.000000\n",
      "18745  -444330148331768170 -1297580205670251233        2.000000\n",
      "34490  6686431125336194142 -4994468824009200256        1.000000\n",
      "27398  3576137684812235192 -5570129644089964821        1.000000\n",
      "...                    ...                  ...             ...\n",
      "20971   692689608292948411  2435024834845042614        2.321928\n",
      "23030  1623838599684589103 -5781461435447152359        1.000000\n",
      "24932  2416280733544962613  7400903238402587728        1.584963\n",
      "31680  4778050608932092852  1642787330067525131        2.000000\n",
      "29484  3829784524040647339  7273139206342584600        1.000000\n",
      "\n",
      "[33240 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('__file__','../../')))\n",
    "import import_ipynb\n",
    "import data_processing\n",
    "import pandas as pd\n",
    "import random\n",
    "from config import configs #import \n",
    "EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = configs.EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS\n",
    "#Top-N accuracy metrics consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.plat_articles = data_processing.reading_content_data()[0]           \n",
    "        self.user_behaviour_full_indexed_df, \\\n",
    "        self.user_behaviour_train_indexed_df, \\\n",
    "        self.user_behaviour_test_indexed_df = data_processing.getting_user_behaviour_indexed()\n",
    "\n",
    "\n",
    "    def get_not_interacted_items_sample(self, person_id, sample_size, seed=42):\n",
    "        interacted_items = data_processing.get_users_content_data(person_id, self.user_behaviour_full_indexed_df)\n",
    "        all_items = set(self.plat_articles['item_id'])\n",
    "        non_interacted_items = all_items - interacted_items\n",
    "\n",
    "        random.seed(seed)\n",
    "        non_interacted_items_sample = random.sample(non_interacted_items, sample_size)\n",
    "        return set(non_interacted_items_sample)\n",
    "\n",
    "    def _verify_hit_top_n(self, item_id, recommended_items, topn):        \n",
    "            try:\n",
    "                index = next(i for i, c in enumerate(recommended_items) if c == item_id)\n",
    "            except:\n",
    "                index = -1\n",
    "            hit = int(index in range(0, topn))\n",
    "            return hit, index\n",
    "\n",
    "    def evaluate_model_for_user(self, model, person_id):\n",
    "        #Getting the items in test set\n",
    "        interacted_values_testset = self.user_behaviour_test_indexed_df.loc[person_id]\n",
    "        if type(interacted_values_testset['item_id']) == pd.Series:\n",
    "            person_interacted_items_testset = set(interacted_values_testset['item_id'])\n",
    "        else:\n",
    "            person_interacted_items_testset = set([int(interacted_values_testset['item_id'])])  \n",
    "        interacted_items_count_testset = len(person_interacted_items_testset) \n",
    "\n",
    "        #Getting a ranked recommendation list from a model for a given user\n",
    "        person_recs_df = model.get_item_recommendations(person_id, \n",
    "                                               items_to_ignore=data_processing.get_users_content_data(person_id, \n",
    "                                                                                    self.user_behaviour_train_indexed_df), \n",
    "                                               topn=10000000000)\n",
    "\n",
    "        hits_at_5_count = 0\n",
    "        hits_at_10_count = 0\n",
    "        #For each item the user has interacted in test set\n",
    "        for item_id in person_interacted_items_testset:\n",
    "            #Getting a random sample (100) items the user has not interacted \n",
    "            #(to represent items that are assumed to be no relevant to the user)\n",
    "            non_interacted_items_sample = self.get_not_interacted_items_sample(person_id, \n",
    "                                                                          sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS, \n",
    "                                                                          seed=item_id%(2**32))\n",
    "\n",
    "            #Combining the current interacted item with the 100 random items\n",
    "            items_to_filter_recs = non_interacted_items_sample.union(set([item_id]))\n",
    "\n",
    "            #Filtering only recommendations that are either the interacted item or from a random sample of 100 non-interacted items\n",
    "            valid_recs_df = person_recs_df[person_recs_df['item_id'].isin(items_to_filter_recs)]                    \n",
    "            valid_recs = valid_recs_df['item_id'].values\n",
    "            #Verifying if the current interacted item is among the Top-N recommended items\n",
    "            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n",
    "            hits_at_5_count += hit_at_5\n",
    "            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n",
    "            hits_at_10_count += hit_at_10\n",
    "\n",
    "        #Recall is the rate of the interacted items that are ranked among the Top-N recommended items, \n",
    "        #when mixed with a set of non-relevant items\n",
    "        recall_at_5 = hits_at_5_count / float(interacted_items_count_testset)\n",
    "        recall_at_10 = hits_at_10_count / float(interacted_items_count_testset)\n",
    "\n",
    "        person_metrics = {'hitrate@5_count':hits_at_5_count, \n",
    "                          'hitrate@10_count':hits_at_10_count, \n",
    "                          'interacted_count': interacted_items_count_testset,\n",
    "                          'recallscore@5': recall_at_5,\n",
    "                          'recallscore@10': recall_at_10}\n",
    "        return person_metrics\n",
    "\n",
    "    def evaluate_model(self, model, recommendation_model_type):\n",
    "        #print('Running evaluation for users')\n",
    "        people_metrics = []\n",
    "        for idx, person_id in enumerate(list(self.user_behaviour_test_indexed_df.index.unique().values)):\n",
    "            #if idx % 100 == 0 and idx > 0:\n",
    "            #    print('%d users processed' % idx)\n",
    "            person_metrics = self.evaluate_model_for_user(model, person_id)  \n",
    "            person_metrics['person_id'] = person_id\n",
    "            people_metrics.append(person_metrics)\n",
    "        print('%d users processed' % idx)\n",
    "\n",
    "        detailed_results_df = pd.DataFrame(people_metrics) \\\n",
    "                            .sort_values('interacted_count', ascending=False)\n",
    "        print(detailed_results_df)\n",
    "        \n",
    "        global_recall_at_5 = detailed_results_df['hitrate@5_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
    "        global_recall_at_10 = detailed_results_df['hitrate@10_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
    "        \n",
    "        global_metrics = {'model_type': recommendation_model_type,\n",
    "                          'recallscore@5': global_recall_at_5,\n",
    "                          'recallscore@10': global_recall_at_10}    \n",
    "        return global_metrics, detailed_results_df\n",
    "\n",
    "    \n",
    "model_evaluator = ModelEvaluator()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
